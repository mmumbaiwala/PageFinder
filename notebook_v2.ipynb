{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bac3c9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'author': '',\n",
      " 'creationDate': \"D:20130826175213+09'00'\",\n",
      " 'creator': 'PFU ScanSnap Manager 6.2.10 #SV600',\n",
      " 'encryption': None,\n",
      " 'format': 'PDF 1.3',\n",
      " 'keywords': '',\n",
      " 'modDate': \"D:20130826175213+09'00'\",\n",
      " 'producer': 'Adobe PDF Scan Library 3.2',\n",
      " 'subject': '',\n",
      " 'title': '',\n",
      " 'trapped': ''}\n",
      "--------------------------------\n",
      "\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "doc = fitz.open(\"SampleData/sv600_c_normal.pdf\")\n",
    "#  doc = fitz.open(\"sv600_c_normal.pdf\")\n",
    "\n",
    "metadata = doc.metadata\n",
    "pprint(metadata)\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "for page in doc:\n",
    "    text = page.get_text()\n",
    "    print(text)\n",
    "    print(\"--------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b553afa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sample Team\\nPrepared By\\nsample-files.com\\nMulti-Page\\nReport\\n“A comprehensive and content-heavy report that\\nincludes text, images, and tables for thorough\\ntesting of pagination and complex layouts.”\\n',\n",
       " 'Table of Contents\\n1. Introduction\\n2. Market Analysis\\n3. Data Analysis\\n4. Product Overview\\n5. Results & Discussion\\n6. Marketing Strategy\\n7. Sales Projections\\n8. Launch Timeline\\nThis sample PDF file is provided by Sample-Files.com. Visit us for more sample files and resource.\\n',\n",
       " 'Introduction\\nThis section introduces the report and highlights the\\nkey objectives. The purpose of this report is to analyze\\ndata, evaluate outcomes, and provide insights for\\nfuture decisions. This analysis is based on various data\\nsources that include quantitative and qualitative\\ninputs. Note: Add an image here illustrating the\\nconcept of data analysis or research methodology. \\nID\\nMetric\\nValue\\nRemarks\\n1\\nMetric 1\\n70\\nValid Data\\n2\\nMetric 2\\n431\\nValid Data\\n3\\nMetric 3\\n186\\nValid Data\\n4\\nMetric 4\\n489\\nValid Data\\n5\\nMetric 5\\n180\\nValid Data\\nThis sample PDF file is provided by Sample-Files.com. Visit us for more sample files and resource.\\n',\n",
       " 'Market Analysis\\nNam quis porta ex. Donec porttitor at sem nec\\nsollicitudin. Ut vel commodo tortor, sagittis egestas\\nnisl. Donec quam mauris, tristique non tempus vitae,\\nornare sed mauris. Etiam blandit tempor metus, at\\nvehicula nisi. Maecenas suscipit vulputate varius.\\nCurrent market share\\nProjected sales for the\\nfirst three years\\nThis sample PDF file is provided by Sample-Files.com. Visit us for more sample files and resource.\\n',\n",
       " 'Data Analysis\\nThis section analyzes data collected from various\\nsources. The data is presented in a structured format\\nto identify trends, patterns, and anomalies. Statistical\\nmethods are used to derive meaningful insights. Note:\\nAdd a chart or a graph here depicting the data trends\\nvisually. \\nThis sample PDF file is provided by Sample-Files.com. Visit us for more sample files and resource.\\n',\n",
       " 'Product Overview\\nNam quis porta ex. Donec porttitor at sem nec\\nsollicitudin. Ut vel commodo tortor, sagittis egestas\\nnisl. Donec quam mauris, tristique non tempus vitae,\\nornare sed mauris. Etiam blandit tempor metus, at\\nvehicula nisi. Maecenas suscipit vulputate varius.\\nInteger et justo velus.\\nUt in ipsum ac risus.\\nMaecenas iaculis.\\nUt nec mauris vel.\\nTellus accumsan.\\nKey Features\\nInteger et justo velus.\\nUt in ipsum ac risus.\\nMaecenas iaculis.\\nUt nec mauris vel.\\nTellus accumsan.\\nKey Features\\nThis sample PDF file is provided by Sample-Files.com. Visit us for more sample files and resource.\\n',\n",
       " 'Results & Discussion\\nThe results and findings are discussed in this section.\\nThe data presented previously is analyzed and\\ncontextualized to understand the implications. This\\nsection highlights key trends, potential causes, and\\nimplications for future strategies. Note: Add a\\ncomparative analysis image or table illustrating\\ndifferent scenarios. \\nThis sample PDF file is provided by Sample-Files.com. Visit us for more sample files and resource.\\n',\n",
       " 'Marketing Strategy\\nTristique non tempus vitae, ornare sed mauris. Etiam\\nblandit tempor metus, at vehicula nisi. Maecenas\\nsuscipit vulputate varius.\\nInteger et justo velus.\\nUt in ipsum ac risus.\\nMaecenas iaculis.\\nUt nec mauris vel.\\nTellus accumsan.\\n1st Strategy\\nInteger et justo velus.\\nUt in ipsum ac risus.\\nMaecenas iaculis.\\nUt nec mauris vel.\\nTellus accumsan.\\n 2nd Strategy\\nThis sample PDF file is provided by Sample-Files.com. Visit us for more sample files and resource.\\n',\n",
       " 'Sales Projections\\nTristique non tempus vitae, ornare sed mauris. Etiam\\nblandit tempor metus, at vehicula nisi. Maecenas\\nsuscipit vulputate varius.\\nMetric\\nSales\\nMarket Size\\n$50 Billion\\nUser Satisfaction\\n85%\\nGrowth Rate\\n10%\\nSeries 1\\nSeries 2\\nItem 1\\nItem 2\\nItem 3\\n0\\n5\\n10\\n15\\n20\\nThis sample PDF file is provided by Sample-Files.com. Visit us for more sample files and resource.\\n',\n",
       " 'Launch Timeline\\nVivamus ac nunc vitae nulla molestie sodales. Proin sit\\namet rhoncus lacus. Cras non erat imperdiet sapien\\nporttitor aliquam nec ut velit.\\nThis sample PDF file is provided by Sample-Files.com. Visit us for more sample files and resource.\\n']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def digital_pdf_get_text(pdf_path:str)->list[str]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = []\n",
    "    for page in doc:\n",
    "        text.append(page.get_text())\n",
    "    return text\n",
    "\n",
    "digital_pdf_get_text(\"SampleData/sample-report_(1).pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8950072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_imageText_with_pdfText(image_text:list[str],\n",
    "                                  pdf_text:list[str])->list[str]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    assert len(image_text) == len(pdf_text)\n",
    "    merged_text = [image_text[i] + pdf_text[i] for i in range(len(image_text))]\n",
    "    return merged_text\n",
    "\n",
    "merge_imageText_with_pdfText(image_text, pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2f5846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_page_finder_result_template(search_conditions):\n",
    "    \"\"\"\n",
    "    Create a template DataFrame for storing page finding results.\n",
    "    \n",
    "    This function generates a standardized template structure for tracking\n",
    "    the results of page finding operations across PDF documents. The template\n",
    "    includes fields for document metadata, page information, and search condition\n",
    "    results.\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame with the following columns:\n",
    "            - Index: Dictionary field for storing page indices\n",
    "            - File_Name: String field for the name of the processed file\n",
    "            - File_Path: String field for the full path to the file\n",
    "            - Page_Count: Integer field for total number of pages in the document\n",
    "            - Page_Number_Found: Integer field for the page number where search\n",
    "              conditions were satisfied\n",
    "            - SearchConditions_Satisfied: Dictionary field for storing boolean\n",
    "              results of each search condition (e.g., {\"A\": True, \"B\": False})\n",
    "    \n",
    "    Example:\n",
    "        >>> template_df = create_page_finder_result_template()\n",
    "        >>> print(template_df.columns.tolist())\n",
    "        ['Index', 'File_Name', 'File_Path', 'Page_Count', 'Page_Number_Found', 'SearchConditions_Satisfied']\n",
    "    \n",
    "    Note:\n",
    "        - The DataFrame is initialized with empty/default values\n",
    "        - SearchConditions_Satisfied field is designed to store results from\n",
    "          search_conditions_document function\n",
    "        - This template serves as a foundation for building result datasets\n",
    "          from multiple document searches\n",
    "    \"\"\"\n",
    "    page_finder_result_template:dict = {\n",
    "        \"Index\":{},\n",
    "        \"File_Name\": \"\",\n",
    "        \"File_Path\": \"\",\n",
    "        \"Page_Count\": 0,\n",
    "        \"Page_Number_Found\": 0,\n",
    "        **search_conditions,\n",
    "        # \"SearchConditions_Satisfied\":{}, #{\"A\":True, \"B\":False},\n",
    "    }\n",
    "    df = pd.DataFrame(page_finder_result_template)\n",
    "    return df\n",
    "\n",
    "create_page_finder_result_template({\"A\":None, \"B\":None, \"C\":None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f642c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_conditions_document(\n",
    "                        doc:list[str],\n",
    "                        search_conditions:dict[str, callable],\n",
    "                        ):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    search_conditions_output_result=[None for _ in doc]\n",
    "\n",
    "    for i, page_text in enumerate(doc):            \n",
    "        search_conditions_output={k:None for k in search_conditions}\n",
    "        \n",
    "        for condition_name, condition_func in search_conditions.items():\n",
    "            if condition_func(\"\", page_text):\n",
    "                search_conditions_output[condition_name]=True\n",
    "            else:\n",
    "                search_conditions_output[condition_name]=False\n",
    "\n",
    "        search_conditions_output_result[i] = search_conditions_output #Use update instead of append for speed\n",
    "    \n",
    "    return search_conditions_output_result\n",
    "\n",
    "def page_find_from_conditions(search_conditions_output:dict)->int:\n",
    "    #TODO: Implement logic\n",
    "    return 4\n",
    "\n",
    "\n",
    "search_conditions = {\n",
    "    \"A\": lambda x, page_text: \"sample\".lower() in page_text.lower(),\n",
    "    \"B\": lambda x, page_text: \"results\" in page_text,\n",
    "    \"C\": lambda x, page_text: \"results\".upper() in page_text.upper(),\n",
    "}\n",
    "doc_texts = [page.get_text() for page in doc]\n",
    "search_conditions_output = search_conditions_document(doc_texts, search_conditions)\n",
    "\n",
    "pprint(search_conditions_output)\n",
    "print(page_find_from_conditions(search_conditions_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71d8b179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "def extract_text_from_image_ocr(image_path_or_bytes,\n",
    "                                tesseract_path=None,\n",
    "                                tesseract_config_mode=\"--psm 4\"\n",
    "                                ):\n",
    "    \"\"\"\n",
    "    Extract text from an image using Tesseract OCR.\n",
    "    \n",
    "    Args:\n",
    "        image_path_or_bytes: Either a file path (str) or image bytes\n",
    "        tesseract_path: Optional path to tesseract executable (Windows users may need this)\n",
    "    \n",
    "    Returns:\n",
    "        str: Extracted text from the image\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If OCR fails or Tesseract is not found\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set tesseract path if provided (common on Windows)\n",
    "        if tesseract_path:\n",
    "            pytesseract.pytesseract.tesseract_cmd = tesseract_path\n",
    "        \n",
    "        # Handle both file paths and image bytes\n",
    "        if isinstance(image_path_or_bytes, str):\n",
    "            # File path provided\n",
    "            image = Image.open(image_path_or_bytes)\n",
    "        else:\n",
    "            # Image bytes provided\n",
    "            image = Image.open(io.BytesIO(image_path_or_bytes))\n",
    "        \n",
    "        # Extract text using OCR\n",
    "        text = pytesseract.image_to_string(image,\n",
    "                                           config=tesseract_config_mode)\n",
    "        \n",
    "        return text.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"OCR failed: {str(e)}\")\n",
    "\n",
    "def extract_text_from_pdf_images_ocr(doc,\n",
    "                                     tesseract_path=None):\n",
    "    \"\"\"\n",
    "    Extract text from all images in a PDF using OCR.\n",
    "    \n",
    "    Args:\n",
    "        doc: PyMuPDF document object\n",
    "        tesseract_path: Optional path to tesseract executable\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary mapping page numbers to lists of extracted text from images\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        images = page.get_images(full=False)\n",
    "        \n",
    "        if not images:\n",
    "            continue\n",
    "            \n",
    "        page_results = []\n",
    "        \n",
    "        for img in images:\n",
    "            xref = img[0]\n",
    "            \n",
    "            try:\n",
    "                # Create pixmap from image\n",
    "                pix = fitz.Pixmap(doc, xref)\n",
    "                \n",
    "                # Convert to PIL Image for OCR\n",
    "                img_data = pix.tobytes(\"png\")\n",
    "                \n",
    "                # Extract text using OCR\n",
    "                extracted_text = extract_text_from_image_ocr(img_data, tesseract_path)\n",
    "                \n",
    "                if extracted_text:\n",
    "                    page_results.append({\n",
    "                        'xref': xref,\n",
    "                        'text': extracted_text,\n",
    "                        'size': (pix.width, pix.height)\n",
    "                    })\n",
    "                \n",
    "                pix = None  # Free memory\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process image {xref} on page {page_num + 1}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if page_results:\n",
    "            # results[page_num] = page_results\n",
    "            results[page_num] = page_results['text']\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Alternative function using easyocr (no Tesseract installation needed)\n",
    "def extract_text_from_image_easyocr(image_path_or_bytes):\n",
    "    \"\"\"\n",
    "    Extract text from an image using EasyOCR (no external dependencies).\n",
    "    \n",
    "    Args:\n",
    "        image_path_or_bytes: Either a file path (str) or image bytes\n",
    "    \n",
    "    Returns:\n",
    "        str: Extracted text from the image\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import easyocr\n",
    "        \n",
    "        # Initialize reader (first time will download models)\n",
    "        reader = easyocr.Reader(['en'])\n",
    "        \n",
    "        # Handle both file paths and image bytes\n",
    "        if isinstance(image_path_or_bytes, str):\n",
    "            # File path provided\n",
    "            result = reader.readtext(image_path_or_bytes)\n",
    "        else:\n",
    "            # Image bytes provided - save temporarily\n",
    "            temp_img = Image.open(io.BytesIO(image_path_or_bytes))\n",
    "            temp_path = \"temp_image.png\"\n",
    "            temp_img.save(temp_path)\n",
    "            result = reader.readtext(temp_path)\n",
    "            import os\n",
    "            os.remove(temp_path)  # Clean up\n",
    "        \n",
    "        # Extract text from results\n",
    "        text = ' '.join([item[1] for item in result])\n",
    "        return text.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"EasyOCR failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "548b3ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 93.8 ms\n",
      "Wall time: 4.21 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'isiness report\\n\\nNew customer\\'s development\\nand increasing the sale of product\\n\\nMy country economy at this season keeps escaping from Odoba of business though holds a crude oil\\nhigh so on unstable element that continues still, and recovering gradually and well.\\nIn the IT industry, there is an influence such as competing intensification in narrowing investment field.\\n\\n[The main product and service at this season]\\n\\nFrom the product headquarters\\n\\nIn the image business, the new model turning on of the A3 high-speed,\\ntwo sided color scanner that achieved a high-speed reading aimed at.\\nwroom was established in United States, Europe, and Asia/Oceania.\\n\\n@image business\\n\\n1) Scanner class\\n\\nA3 high-speed, two sided color scanner \"fi-5900C\" that 100 high-n\\nfunction to enable industry-leading was installed was announced in\\nScanSnap gotten popular because of an office and individual use.\\n\\n2) DLM solution scanner\\n\\nThe DLM solution that used received the rise of the concern to efficient\\nmanagement and internal management of the corporate private circum-\\n\\nusually bad\\n\\nSatisfaction rating to new product\\n\\nstances report in recent years and attracted attention. The function of software that the inspection of data is possible by\\nthe sense that turns over the file is strengthened, and easiness to use has been improved.\\n\\n[approach on business risk]\\n\\nIn-house activity\\n\\nThe attestation intended for each office in Shinbashi, Kansai, and Tokai was acquired in environment ISO in February,\\n2006. In addition, it participates in the minus 6% that is a national movement of the global warming prevention, and\\n\"Culbiz\" is done. The scandal of the enterprise has frequently generated is received, concern is sent to the system mainte-\\n\\nnance including the observance of the law in recent years.\\n\\nEnhancement of system of management\\n\\nThe committee that aimed at the decrease of a variety of business\\nrisks in an individual business talk was newly established. Moreover,\\nthe recognition of \"Privacy mark\" is received to manage customer and\\nemployee\\'s individual information appropriately in 2001, and the activ-\\nity based on the protection of individual information policy is continued.\\nIt is ..bAsia/Oceania in globalln addition, our technology, commodity\\npower, and correspondence power were evaluating acquired.\\n\\n40)\\n\\n30)\\n\\n20\\n\\n10\\nol\\n\\n1998 1999 2000 2001 2002 2003 2004\\n\\nSatisfaction rating to new product'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tesseract_path = r\"C:\\Users\\mmumbaiwala\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe\"\n",
    "# Example 1: OCR on a specific image\n",
    "image_text = extract_text_from_image_ocr(\"image_5.png\",\n",
    "                                         tesseract_path=tesseract_path,\n",
    "                                         tesseract_config_mode=\"--psm 1\")\n",
    "\n",
    "image_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a040acb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m doc = fitz.open(\u001b[33m\"\u001b[39m\u001b[33mSampleData/sv600_c_normal.pdf\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mextract_text_from_pdf_images_ocr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mtesseract_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtesseract_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 95\u001b[39m, in \u001b[36mextract_text_from_pdf_images_ocr\u001b[39m\u001b[34m(doc, tesseract_path)\u001b[39m\n\u001b[32m     91\u001b[39m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m page_results:\n\u001b[32m     94\u001b[39m         \u001b[38;5;66;03m# results[page_num] = page_results\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         results[page_num] = \u001b[43mpage_results\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[31mTypeError\u001b[39m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "doc = fitz.open(\"SampleData/sv600_c_normal.pdf\")\n",
    "extract_text_from_pdf_images_ocr(doc=doc,\n",
    "                                 tesseract_path=tesseract_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec3b8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d63a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Example 2: OCR on all images in your PDF\n",
    "# For Windows users who installed Tesseract to default location:\n",
    "\n",
    "ocr_results = extract_text_from_pdf_images_ocr(doc, tesseract_path)\n",
    "\n",
    "# Example 3: Using EasyOCR (no Tesseract needed)\n",
    "# easyocr_text = extract_text_from_image_easyocr(\"image_5.png\")\n",
    "\n",
    "# Print results\n",
    "for page_num, images in ocr_results.items():\n",
    "    print(f\"\\nPage {page_num + 1}:\")\n",
    "    for img_info in images:\n",
    "        print(f\"  Image {img_info['xref']}: {img_info['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d1c2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from attrs import define, field, validators\n",
    "# from typing import Optional, Tuple\n",
    "\n",
    "# @define\n",
    "# class StaticTextElement:\n",
    "#     search_text: str = field(validator=validators.instance_of(str))\n",
    "#     max_errors: int = field(validator=validators.instance_of(int))\n",
    "#     max_error_rate: float = field(validator=validators.instance_of(float))\n",
    "#     match_case: bool = field(validator=validators.instance_of(bool))\n",
    "#     # region: Optional[Tuple[int, int, int, int]] = None  # (x1, y1, x2, y2) bounding box\n",
    "#     # use_regex: bool = False  # Support regex search\n",
    "\n",
    "# @define\n",
    "# class MatchResult:\n",
    "#     search_text: str = field(validator=validators.instance_of(str))\n",
    "#     errors: int = field(validator=validators.instance_of(int))\n",
    "#     error_rate: float = field(validator=validators.instance_of(float))\n",
    "#     match_case: bool = field(validator=validators.instance_of(bool))\n",
    "#     success: bool = field(validator=validators.instance_of(bool))\n",
    "#     # matched_region: Optional[Tuple[int, int, int, int]] = None\n",
    "#     # confidence: Optional[float] = None  # Confidence score from OCR or matching algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d319bf62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StaticTextElement(search_text='sample', max_errors=1, max_error_rate=0.5, match_case=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# StaticTextElement(search_text=\"sample\",\n",
    "#                   max_errors=1,\n",
    "#                   max_error_rate=0.5,\n",
    "#                   match_case=True\n",
    "#                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c1d469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MatchResult(search_text='SuperSpecial_secret', errors=0, error_rate=0.0, match_case=False, success=True)\n",
      "MatchResult(search_text='AnotherText', errors=1, error_rate=0.08333333333333333, match_case=False, success=True)\n",
      "MatchResult(search_text='book', errors=1, error_rate=0.25, match_case=False, success=False)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# from rapidfuzz import fuzz, distance\n",
    "# from attrs import define, field, validators\n",
    "# from typing import Optional, Tuple\n",
    "\n",
    "# @define\n",
    "# class StaticTextElement:\n",
    "#     search_text: str = field(validator=validators.instance_of(str))\n",
    "#     max_errors: int = field(validator=validators.instance_of(int))\n",
    "#     max_error_rate: float = field(validator=validators.instance_of(float))\n",
    "#     match_case: bool = field(validator=validators.instance_of(bool))\n",
    "\n",
    "# @define\n",
    "# class MatchResult:\n",
    "#     search_text: str = field(validator=validators.instance_of(str))\n",
    "#     errors: int = field(validator=validators.instance_of(int))\n",
    "#     error_rate: float = field(validator=validators.instance_of(float))\n",
    "#     match_case: bool = field(validator=validators.instance_of(bool))\n",
    "#     success: bool = field(validator=validators.instance_of(bool))\n",
    "\n",
    "# def search_static_text_elements(elements: list[StaticTextElement],\n",
    "#                                 text: str,\n",
    "#                                 max_hypothesis: int = 3,\n",
    "#                                 max_window_size: int = 11,\n",
    "#                                 debug_mode: bool = False) -> list[MatchResult]:\n",
    "#     \"\"\"\n",
    "#     Search for static text elements with fuzzy matching.\n",
    "    \n",
    "#     Args:\n",
    "#         elements: List of StaticTextElement to search for\n",
    "#         text: Text to search within\n",
    "#         max_hypothesis: Maximum number of best matches to keep for each element\n",
    "#         max_window_size: Additional characters to add to pattern length for window size\n",
    "#         debug_mode: If True, print detailed search process. If False, print clean output.\n",
    "    \n",
    "#     Returns:\n",
    "#         List of MatchResult objects (one per element, with best match)\n",
    "#     \"\"\"\n",
    "#     results = []\n",
    "\n",
    "#     for element in elements:\n",
    "#         if element.match_case:\n",
    "#             text_to_search = text\n",
    "#             pattern = element.search_text\n",
    "#         else:\n",
    "#             text_to_search = text.lower()\n",
    "#             pattern = element.search_text.lower()\n",
    "\n",
    "#         pattern_len = len(pattern)\n",
    "#         window_size = pattern_len + max_window_size  # allow some flexibility\n",
    "        \n",
    "#         # Keep track of top N hypotheses for this element\n",
    "#         hypotheses = []  # List of (score, errors, substring) tuples\n",
    "        \n",
    "#         if debug_mode:\n",
    "#             print(f\"\\n=== Searching for '{element.search_text}' ===\")\n",
    "#             print(f\"Pattern (normalized): '{pattern}' (length: {pattern_len})\")\n",
    "#             print(f\"Window size: {window_size}\")\n",
    "#             print(f\"Max hypotheses to keep: {max_hypothesis}\")\n",
    "\n",
    "#         # Quick exact match shortcut\n",
    "#         if pattern in text_to_search:\n",
    "#             if debug_mode:\n",
    "#                 print(f\"✓ Exact match found!\")\n",
    "#             results.append(MatchResult(\n",
    "#                 search_text=element.search_text,\n",
    "#                 errors=0,\n",
    "#                 error_rate=0.0,\n",
    "#                 match_case=element.match_case,\n",
    "#                 success=True\n",
    "#             ))\n",
    "#             continue\n",
    "\n",
    "#         # Sliding window fuzzy search\n",
    "#         for i in range(len(text_to_search) - window_size + 1):\n",
    "#             snippet = text_to_search[i:i+window_size]\n",
    "            \n",
    "#             # Use partial_ratio for scoring\n",
    "#             score = fuzz.partial_ratio(pattern, snippet)\n",
    "            \n",
    "#             # Find the best substring within the snippet - try different lengths\n",
    "#             best_substring_in_snippet = None\n",
    "#             best_substring_score = -1\n",
    "            \n",
    "#             # Try different substring lengths around the pattern length\n",
    "#             # This allows us to find complete words that might be longer/shorter than pattern\n",
    "#             for length_offset in range(-2, 4):  # Try lengths: pattern_len-2, pattern_len-1, pattern_len, pattern_len+1, pattern_len+2\n",
    "#                 target_length = pattern_len + length_offset\n",
    "#                 if target_length <= 0 or target_length > len(snippet):\n",
    "#                     continue\n",
    "                    \n",
    "#                 for j in range(len(snippet) - target_length + 1):\n",
    "#                     sub_snippet = snippet[j:j + target_length]\n",
    "#                     sub_score = fuzz.ratio(pattern, sub_snippet)\n",
    "                    \n",
    "#                     if sub_score > best_substring_score:\n",
    "#                         best_substring_score = sub_score\n",
    "#                         best_substring_in_snippet = sub_snippet\n",
    "            \n",
    "#             # Calculate Levenshtein distance for the best substring\n",
    "#             if best_substring_in_snippet:\n",
    "#                 errors = distance.Levenshtein.distance(pattern, best_substring_in_snippet)\n",
    "                \n",
    "#                 # Create hypothesis tuple: (score, errors, substring, position)\n",
    "#                 hypothesis = (score, errors, best_substring_in_snippet, i)\n",
    "                \n",
    "#                 # Check if this hypothesis is worth keeping\n",
    "#                 should_add = False\n",
    "                \n",
    "#                 if len(hypotheses) < max_hypothesis:\n",
    "#                     # Always add if we haven't reached max_hypothesis\n",
    "#                     should_add = True\n",
    "#                 else:\n",
    "#                     # Check if this hypothesis is better than any existing ones\n",
    "#                     worst_hypothesis = max(hypotheses, key=lambda x: (x[1], -x[0]))  # Worst by errors, then by score\n",
    "#                     if errors < worst_hypothesis[1] or (errors == worst_hypothesis[1] and score > worst_hypothesis[0]):\n",
    "#                         should_add = True\n",
    "                \n",
    "#                 if should_add:\n",
    "#                     # Add new hypothesis\n",
    "#                     hypotheses.append(hypothesis)\n",
    "                    \n",
    "#                     # Keep only top max_hypothesis hypotheses\n",
    "#                     if len(hypotheses) > max_hypothesis:\n",
    "#                         # Sort by errors (ascending), then by score (descending)\n",
    "#                         hypotheses.sort(key=lambda x: (x[1], -x[0]))\n",
    "#                         hypotheses = hypotheses[:max_hypothesis]\n",
    "                    \n",
    "#                     if debug_mode:\n",
    "#                         print(f\"  New hypothesis added: score={score:.1f}, errors={errors}, substring='{best_substring_in_snippet}', pos={i}\")\n",
    "#                         print(f\"  Current top {len(hypotheses)} hypotheses:\")\n",
    "#                         for idx, (s, e, sub, pos) in enumerate(hypotheses):\n",
    "#                             print(f\"    {idx+1}. Score: {s:.1f}, Errors: {e}, Substring: '{sub}', Position: {pos}\")\n",
    "\n",
    "#         # Select the best hypothesis (lowest errors, then highest score)\n",
    "#         if hypotheses:\n",
    "#             # Sort by errors (ascending), then by score (descending)\n",
    "#             hypotheses.sort(key=lambda x: (x[1], -x[0]))\n",
    "#             best_score, best_errors, best_substring, best_position = hypotheses[0]\n",
    "            \n",
    "#             if debug_mode:\n",
    "#                 print(f\"\\n  Best hypothesis selected:\")\n",
    "#                 print(f\"    Score: {best_score:.1f}, Errors: {best_errors}, Substring: '{best_substring}', Position: {best_position}\")\n",
    "            \n",
    "#             # Calculate error rate based on actual substring length\n",
    "#             actual_length = len(best_substring) if best_substring else pattern_len\n",
    "#             error_rate = best_errors / max(1, actual_length)\n",
    "            \n",
    "#             success = (\n",
    "#                 best_errors <= element.max_errors and\n",
    "#                 error_rate <= element.max_error_rate\n",
    "#             )\n",
    "            \n",
    "#             if debug_mode:\n",
    "#                 print(f\"  Final result: errors={best_errors}, error_rate={error_rate:.4f}, success={success}\")\n",
    "            \n",
    "#             results.append(MatchResult(\n",
    "#                 search_text=element.search_text,\n",
    "#                 errors=best_errors,\n",
    "#                 error_rate=error_rate,\n",
    "#                 match_case=element.match_case,\n",
    "#                 success=success\n",
    "#             ))\n",
    "#         else:\n",
    "#             if debug_mode:\n",
    "#                 print(f\"  No hypotheses found!\")\n",
    "#             results.append(MatchResult(\n",
    "#                 search_text=element.search_text,\n",
    "#                 errors=-1,\n",
    "#                 error_rate=1.0,\n",
    "#                 match_case=element.match_case,\n",
    "#                 success=False\n",
    "#             ))\n",
    "\n",
    "#     return results\n",
    "# # Example usage:\n",
    "# if __name__ == \"__main__\":\n",
    "#     elements = [\n",
    "#         StaticTextElement(search_text=\"SuperSpecial_secret\",\n",
    "#                           max_errors=3,\n",
    "#                           max_error_rate=0.2,\n",
    "#                           match_case=False\n",
    "#                           ),\n",
    "#         StaticTextElement(search_text=\"AnotherText\",\n",
    "#                           max_errors=3,\n",
    "#                           max_error_rate=0.5,\n",
    "#                           match_case=False\n",
    "#                           ),\n",
    "#         StaticTextElement(search_text=\"book\",\n",
    "#                           max_errors=3,\n",
    "#                           max_error_rate=0.1,\n",
    "#                           match_case=False\n",
    "#                           )\n",
    "#     ]\n",
    "\n",
    "#     ocr_text = \"Some big noisy OCR output that includes Superspecial_secret and other boak AnnotherText ...\"\n",
    "#     matches = search_static_text_elements(elements, ocr_text)\n",
    "\n",
    "#     for match in matches:\n",
    "#         print(match)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c055be4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
